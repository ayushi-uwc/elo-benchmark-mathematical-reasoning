# ğŸ† Mathematical Reasoning Language Model Elo Rating

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![MongoDB](https://img.shields.io/badge/Database-MongoDB-green.svg)](https://www.mongodb.com/)
[![LiteLLM](https://img.shields.io/badge/LLM-LiteLLM-orange.svg)](https://github.com/BerriAI/litellm)
[![arXiv](https://img.shields.io/badge/arXiv-2024.xxxxx-b31b1b.svg)](https://arxiv.org/abs/2024.xxxxx)

*A peer-federated evaluation framework addressing benchmark overfitting and cost-efficiency trade-offs in large language model assessment*

[ğŸ“ˆ Results](#-results) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ”§ Installation](#-installation)

</div>

---

```
DETAILED LEADERBOARD - Mathematical Reasoning Tournament Results
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # â”‚Model                             â”‚Raw ELO â”‚Cost ELOâ”‚ Raw Avg  â”‚ Cost Avg â”‚ W-L-D      â”‚ Tokens â”‚ Cost $    â”‚Matches â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 â”‚GPT-o4-mini                       â”‚ 1571.2 â”‚ 1570.3 â”‚  0.9459  â”‚  0.9382  â”‚ 51-4-0     â”‚ 645572 â”‚ $1.52775  â”‚   11   â”‚
â”‚ 2 â”‚GPT-o3-mini                       â”‚ 1538.0 â”‚ 1533.2 â”‚  0.8576  â”‚  0.8071  â”‚ 30-5-0     â”‚ 350973 â”‚ $7.08507  â”‚   7    â”‚
â”‚ 3 â”‚Qwen 3 32B                        â”‚ 1537.1 â”‚ 1537.8 â”‚  0.7889  â”‚  0.7938  â”‚ 34-8-3     â”‚ 537244 â”‚ $0.01924  â”‚   9    â”‚
â”‚ 4 â”‚GPT-4.1 mini                      â”‚ 1531.3 â”‚ 1532.6 â”‚  0.7861  â”‚  0.7988  â”‚ 25-5-5     â”‚ 581935 â”‚ $0.44733  â”‚   7    â”‚
â”‚ 5 â”‚Grok 3 Mini Fast                  â”‚ 1529.0 â”‚ 1530.1 â”‚  0.7576  â”‚  0.7654  â”‚ 28-8-3     â”‚ 296422 â”‚ $0.23720  â”‚   8    â”‚
â”‚ 6 â”‚GPT-o3                            â”‚ 1524.7 â”‚ 1519.5 â”‚  0.9458  â”‚  0.9124  â”‚ 50-1-4     â”‚ 592718 â”‚ $14.87402 â”‚   11   â”‚
â”‚ 7 â”‚Grok 3 Mini                       â”‚ 1521.8 â”‚ 1521.9 â”‚  0.6763  â”‚  0.6760  â”‚ 25-12-2    â”‚ 168843 â”‚ $0.03434  â”‚   8    â”‚
â”‚ 8 â”‚Grok 3                            â”‚ 1520.0 â”‚ 1518.5 â”‚  0.6281  â”‚  0.6196  â”‚ 34-20-1    â”‚ 408658 â”‚ $2.86888  â”‚   11   â”‚
â”‚ 9 â”‚Claude 3.5 Haiku                  â”‚ 1519.6 â”‚ 1519.2 â”‚  0.6548  â”‚  0.6513  â”‚ 29-15-1    â”‚ 146254 â”‚ $0.18823  â”‚   9    â”‚
â”‚10 â”‚GPT-4.1                           â”‚ 1519.0 â”‚ 1521.3 â”‚  0.6713  â”‚  0.6931  â”‚ 21-9-5     â”‚ 504876 â”‚ $1.86800  â”‚   7    â”‚
â”‚11 â”‚Meta LLama 4 Maverick Instruct 17Bâ”‚ 1514.7 â”‚ 1515.7 â”‚  0.6680  â”‚  0.6772  â”‚ 30-15-0    â”‚ 25773  â”‚ $0.00277  â”‚   9    â”‚
â”‚12 â”‚Claude 3 Opus                     â”‚ 1514.5 â”‚ 1508.5 â”‚  0.6261  â”‚  0.5739  â”‚ 24-14-1    â”‚ 296314 â”‚ $6.80181  â”‚   8    â”‚
â”‚13 â”‚DeepSeek R1 Distill Llama 70B    â”‚ 1514.0 â”‚ 1514.2 â”‚  0.6011  â”‚  0.6044  â”‚ 26-17-2    â”‚ 74835  â”‚ $0.06732  â”‚   9    â”‚
â”‚14 â”‚Grok 3 Fast                       â”‚ 1507.8 â”‚ 1503.2 â”‚  0.5425  â”‚  0.5128  â”‚ 26-21-8    â”‚ 118245 â”‚ $1.38207  â”‚   11   â”‚
â”‚15 â”‚Gemini 2.0 Flash                  â”‚ 1507.2 â”‚ 1507.8 â”‚  0.5587  â”‚  0.5651  â”‚ 23-18-3    â”‚ 91137  â”‚ $0.01453  â”‚   9    â”‚
â”‚16 â”‚Mistral Saba 24B                  â”‚ 1500.9 â”‚ 1500.8 â”‚  0.5174  â”‚  0.5156  â”‚ 9-9-1      â”‚ 11253  â”‚ $0.00889  â”‚   4    â”‚
â”‚17 â”‚Meta LLama 4 Scout Instruct 17B   â”‚ 1498.5 â”‚ 1498.7 â”‚  0.4982  â”‚  0.5008  â”‚ 19-19-2    â”‚ 66809  â”‚ $0.00590  â”‚   8    â”‚
â”‚18 â”‚GPT-4o                            â”‚ 1498.2 â”‚ 1493.3 â”‚  0.5076  â”‚  0.4782  â”‚ 28-27-5    â”‚ 287981 â”‚ $2.13102  â”‚   12   â”‚
â”‚19 â”‚Qwen 3.2 235B                     â”‚ 1496.6 â”‚ 1497.3 â”‚  0.5453  â”‚  0.5511  â”‚ 24-20-1    â”‚ 75909  â”‚ $0.00293  â”‚   9    â”‚
â”‚20 â”‚Microsoft Phi 4                   â”‚ 1495.1 â”‚ 1496.6 â”‚  0.4704  â”‚  0.4811  â”‚ 21-24-5    â”‚ 86930  â”‚ $0.00266  â”‚   10   â”‚
â”‚21 â”‚Gemini 2.0 Flash Lite             â”‚ 1493.1 â”‚ 1493.5 â”‚  0.4495  â”‚  0.4509  â”‚ 19-23-2    â”‚ 10547  â”‚ $0.00149  â”‚   9    â”‚
â”‚22 â”‚Grok 2                            â”‚ 1489.2 â”‚ 1489.1 â”‚  0.4219  â”‚  0.4221  â”‚ 18-25-2    â”‚ 64056  â”‚ $0.36185  â”‚   9    â”‚
â”‚23 â”‚Gemma 3 27B                       â”‚ 1488.1 â”‚ 1491.1 â”‚  0.3977  â”‚  0.4269  â”‚ 15-23-1    â”‚ 36755  â”‚ $0.00583  â”‚   8    â”‚
â”‚24 â”‚GPT-4.1 nano                      â”‚ 1485.9 â”‚ 1492.2 â”‚  0.4510  â”‚  0.4987  â”‚ 23-26-1    â”‚ 48845  â”‚ $0.00767  â”‚   10   â”‚
â”‚25 â”‚Gemma 3 12B                       â”‚ 1485.0 â”‚ 1486.6 â”‚  0.3956  â”‚  0.4083  â”‚ 18-27-0    â”‚ 13337  â”‚ $0.00000  â”‚   9    â”‚
â”‚26 â”‚Gemma 3 4B                        â”‚ 1475.8 â”‚ 1476.1 â”‚  0.2574  â”‚  0.2604  â”‚ 9-26-0     â”‚ 11411  â”‚ $0.00000  â”‚   7    â”‚
â”‚27 â”‚Claude 3.5 Sonnet                 â”‚ 1475.6 â”‚ 1475.0 â”‚  0.2968  â”‚  0.2923  â”‚ 9-26-3     â”‚ 27471  â”‚ $0.14046  â”‚   8    â”‚
â”‚28 â”‚Gemma 2 9B                        â”‚ 1468.7 â”‚ 1469.2 â”‚  0.2860  â”‚  0.2906  â”‚ 10-24-0    â”‚ 42799  â”‚ $0.00856  â”‚   7    â”‚
â”‚29 â”‚Gemini 1.5 Flash 8B               â”‚ 1465.3 â”‚ 1468.8 â”‚  0.2326  â”‚  0.2588  â”‚ 10-34-1    â”‚ 14102  â”‚ $0.00087  â”‚   9    â”‚
â”‚30 â”‚Claude 3 Haiku                    â”‚ 1463.0 â”‚ 1466.0 â”‚  0.1767  â”‚  0.2048  â”‚ 5-31-4     â”‚ 17058  â”‚ $0.00862  â”‚   8    â”‚
â”‚31 â”‚GPT-3.5 Turbo                     â”‚ 1462.4 â”‚ 1462.4 â”‚  0.1566  â”‚  0.1557  â”‚ 5-28-1     â”‚ 28949  â”‚ $0.01761  â”‚   7    â”‚
â”‚32 â”‚LLaMA 3.1 8B Instant              â”‚ 1461.8 â”‚ 1462.0 â”‚  0.2646  â”‚  0.2651  â”‚ 13-39-3    â”‚ 34788  â”‚ $0.00202  â”‚   11   â”‚
â”‚33 â”‚Llama 3.3 70B                     â”‚ 1448.1 â”‚ 1448.5 â”‚  0.1450  â”‚  0.1468  â”‚ 6-41-2     â”‚ 22915  â”‚ $0.01436  â”‚   10   â”‚
â”‚34 â”‚Allamanda 2 7B                    â”‚ 1446.9 â”‚ 1447.0 â”‚  0.0000  â”‚  0.0000  â”‚ 0-35-0     â”‚ 57477  â”‚ $0.00832  â”‚   7    â”‚
â”‚35 â”‚Gemma 3 1B                        â”‚ 1442.0 â”‚ 1442.0 â”‚  0.0238  â”‚  0.0245  â”‚ 1-39-0     â”‚ 12268  â”‚ $0.00000  â”‚   8    â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Detailed Logs have been uploaded to google drive

[Detailed Logs](https://drive.google.com/drive/folders/1-43KtZsh6r_DBmSARjDxcEJkf1iw-ADR?usp=sharing)

*The above table represents the complete mathematical reasoning tournament results, showing the actual performance rankings from our comprehensive evaluation.*

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- MongoDB database
- API keys for LLM providers

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys and MongoDB URI
```

### Run Your First Tournament

```bash
# Start a tournament with default settings
python main.py

# Run with custom parameters
python main.py --batch-size 10 --stats

# Check model health
python check_models.py
```

## ğŸ› ï¸ Installation

### Environment Setup

1. **Clone Repository**
```bash
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure Environment**
Create a `.env` file:
```env
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
HUGGINGFACE_API_KEY=your_huggingface_key
GROQ_API_KEY=your_groq_key
XAI_API_KEY=your_xai_key

# Database Configuration
MONGODB_URI=your_mongodb_connection_string
```

### Model Configuration

Add new models in `model_definitions.py`:

```python
{
    "name": "GPT-4 Turbo",
    "model_id": "gpt-4-turbo-preview",
    "provider": "openai",
    "input_cost_per_million": 10.0,
    "output_cost_per_million": 30.0,
    "pricing_source": "OpenAI API Pricing"
}
```

## ğŸ“Š Usage

### Command Line Interface

```bash
# Basic tournament run
python main.py

# Advanced options
python main.py \
    --max-matches 100 \
    --batch-size 10 \
    --stats \
    --log-level INFO
```

### Programmatic Usage

```python
from models import initialize_models
from tournament import run_tournament_matches
from model_definitions import MODELS

# Initialize models
models = initialize_models(MODELS)

# Run tournament
matches = run_tournament_matches(models, max_matches=50)

# Analyze results
for match in matches:
    print(f"Match: {match.participants}")
    print(f"Scores: {match.judgment['raw_score']}")
```

### Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_matches` | 50 | Maximum matches per model |
| `batch_size` | 10 | Matches per batch |
| `k_factor` | 32 | Elo update rate |
| `cost_temperature` | 0.05 | Cost sensitivity |
| `judge_temperature` | 300 | Judge weight temperature |

## ğŸ“ Project Structure

```
elo-benchmark/
â”œâ”€â”€ ğŸ“„ main.py                    # Main entry point
â”œâ”€â”€ ğŸ† tournament.py              # Tournament management and pairing
â”œâ”€â”€ ğŸ¤– models.py                  # LLM model classes and Elo tracking
â”œâ”€â”€ âš”ï¸ matches.py                 # Match logic and prompt templates
â”œâ”€â”€ ğŸ—„ï¸ database.py                # MongoDB operations and data persistence
â”œâ”€â”€ âš™ï¸ config.py                  # Configuration and environment variables
â”œâ”€â”€ ğŸ“‹ model_definitions.py       # Model specifications and pricing
â”œâ”€â”€ ğŸ“Š leaderboard.py             # Results display and ranking
â”œâ”€â”€ ğŸ” check_models.py            # Model health checks and validation
â”œâ”€â”€ ğŸ“ˆ match_results_table.py     # Results analysis and visualization
â”œâ”€â”€ ğŸ”§ logger_config.py           # Logging configuration
â”œâ”€â”€ ğŸ“ logs/                      # Tournament logs and match history
â”œâ”€â”€ ğŸ§ª tests/                     # Unit tests and integration tests
â””â”€â”€ ğŸ“š docs/                      # Additional documentation
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/ -v

# Check code style
black . && flake8 . && mypy .
```

### Research Contributions

We particularly welcome:
- Novel evaluation methodologies
- Mathematical improvements to the rating system
- New domain adaptations
- Efficiency optimizations
- Empirical studies and analysis

## ğŸ“„ License

This project is licensed under the ([![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
).

## ğŸ™ Acknowledgments

- Built with [LiteLLM](https://github.com/BerriAI/litellm) for unified LLM access
- Inspired by chess Elo rating systems and [TrueSkill](https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/)
- Mathematical framework based on [Bradley-Terry models](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)
- Mathematical evaluation methodology inspired by educational assessment standards
- Special thanks to the open-source AI research community


<div align="center">

</div>
