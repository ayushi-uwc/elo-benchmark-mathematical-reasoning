# ğŸ† UNER: United Nasscom Elo Rating

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-sa/4.0/)
[![MongoDB](https://img.shields.io/badge/Database-MongoDB-green.svg)](https://www.mongodb.com/)
[![LiteLLM](https://img.shields.io/badge/LLM-LiteLLM-orange.svg)](https://github.com/BerriAI/litellm)

*An advanced peer-federated evaluation framework for large language models*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– How It Works](#-how-it-works) â€¢ [ğŸ”§ Installation](#-installation) â€¢ [ğŸ¤ Contributing](#-contributing)

</div>

---

## ğŸ¯ Overview

**UNER** is a revolutionary evaluation framework that solves two critical problems in LLM assessment:

- ğŸª **Benchmark Overfitting**: Traditional static benchmarks become stale as models game them
- ğŸ’° **Cost Blindness**: Performance metrics ignore real-world computational costs

### âœ¨ Key Innovations

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Dynamic Challenge Generation** | Fresh prompts generated by top models each cycle |
| âš–ï¸ **Peer-Federated Judging** | Models judge each other with ELO-weighted votes |
| ğŸ“Š **Dual-Track ELO Ratings** | Separate ratings for quality and cost-efficiency |
| ğŸ… **Swiss Tournament Pairing** | Fair matchups within ELO windows |
| ğŸ“ **Immutable Logging** | Complete transparency and reproducibility |

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- MongoDB database
- API keys for LLM providers

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your API keys and MongoDB URI
```

### Run Your First Tournament

```bash
# Start a tournament with default settings
python main.py

# Run with custom parameters
python main.py --batch-size 10 --stats

# Check model health
python check_models.py
```

## ğŸ“Š How It Works

### ğŸ† ELO Rating System

UNER maintains **two separate ELO ratings** for each model:

#### Raw Performance Rating
Based purely on judge votes and match outcomes

#### Cost-Adjusted Rating  
Incorporates computational costs into performance evaluation

### ğŸ­ Match Structure

1. **ğŸ¯ Model Pairing**: Swiss-style pairing within ELO windows
2. **ğŸ“ Case Generation**: Top model creates clinical scenario
3. **â“ Question Generation**: Another top model poses question
4. **ğŸ’­ Response Collection**: Competing models answer
5. **âš–ï¸ Judgment**: Peer models evaluate responses
6. **ğŸ“ˆ ELO Updates**: Ratings adjusted based on weighted votes

## ğŸ› ï¸ Configuration

### Environment Variables

Create a `.env` file with your credentials:

```env
# LLM Provider API Keys
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
HUGGINGFACE_API_KEY=your_huggingface_key
GROQ_API_KEY=your_groq_key
XAI_API_KEY=your_xai_key

# Database
MONGODB_URI=your_mongodb_connection_string
```

### Adding New Models

Edit `model_definitions.py`:

```python
{
    "name": "GPT-4 Turbo",
    "model_id": "gpt-4-turbo-preview", 
    "provider": "openai",
    "input_cost_per_million": 10.0,
    "output_cost_per_million": 30.0,
    "pricing_source": "OpenAI API Pricing"
}
```

## ğŸ“ Project Structure

```
elo-benchmark/
â”œâ”€â”€ ğŸ“„ main.py              # Main entry point
â”œâ”€â”€ ğŸ† tournament.py        # Tournament management
â”œâ”€â”€ ğŸ¤– models.py            # LLM model classes
â”œâ”€â”€ âš”ï¸ matches.py           # Match logic & prompts
â”œâ”€â”€ ğŸ—„ï¸ database.py          # MongoDB operations
â”œâ”€â”€ âš™ï¸ config.py            # Configuration
â”œâ”€â”€ ğŸ“‹ model_definitions.py # Model specifications
â”œâ”€â”€ ğŸ“Š leaderboard.py       # Results display
â”œâ”€â”€ ğŸ” check_models.py      # Health checks
â””â”€â”€ ğŸ“ logs/               # Tournament logs
```

## ğŸ® Command Line Options

| Option | Description | Example |
|--------|-------------|---------|
| `--max-matches` | Maximum matches to run | `--max-matches 100` |
| `--batch-size` | Matches per batch | `--batch-size 10` |
| `--stats` | Show detailed statistics | `--stats` |
| `--stats-only` | Only show stats, no matches | `--stats-only` |
| `--log-level` | Set logging verbosity | `--log-level DEBUG` |

## ğŸ“ˆ Sample Output

```
DETAILED LEADERBOARD
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ # â”‚Model             â”‚Raw ELO â”‚Cost ELOâ”‚Raw Avg   â”‚Cost Avg  â”‚W-L-D   â”‚Tokens  â”‚Cost $     â”‚Matches â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1 â”‚GPT-4 Turbo       â”‚  1687.2â”‚  1654.1â”‚    0.6234â”‚    0.5987â”‚  12-8-2â”‚   45231â”‚  $0.12456â”‚      22â”‚
â”‚ 2 â”‚Claude-3 Opus     â”‚  1623.8â”‚  1598.7â”‚    0.5876â”‚    0.5654â”‚  10-9-3â”‚   38976â”‚  $0.09876â”‚      22â”‚
â”‚ 3 â”‚Gemini Pro        â”‚  1534.5â”‚  1567.9â”‚    0.5123â”‚    0.5345â”‚   8-12-2â”‚   32145â”‚  $0.06543â”‚      22â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Methodology Deep Dive

<details>
<summary>Click to expand detailed methodology</summary>

### 1. Model Registration
- Initialize metadata and baseline ELO (1500)
- Configure cost tracking and API endpoints

### 2. Fair Matching  
- Swiss-style pairing within ELO strata
- Prevent repeated matchups
- Balance computational costs

### 3. Challenge Creation
- Top models generate novel prompts
- Committee validation (80% approval threshold)
- Domain-agnostic framework

### 4. Solution Generation
- Standardized decoding parameters
- Comprehensive cost tracking
- Latency measurement

### 5. Peer Evaluation
- ELO-weighted judge selection
- Softmax vote weighting
- Binary outcome determination

### 6. Dual Scoring
- Raw performance calculation
- Cost-adjusted score computation
- ELO rating updates

### 7. Rating Propagation
- Gaussian kernel smoothing
- Cohesive rating evolution
- Stability maintenance

### 8. Logging & Dissemination
- Immutable audit trail
- Periodic leaderboards
- Performance analytics

</details>

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/yourusername/elo-benchmark.git
cd elo-benchmark
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/

# Check code style
black . && flake8 .
```

## ğŸ“„ License

This project is licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](LICENSE).

## ğŸ™ Acknowledgments

- Built with [LiteLLM](https://github.com/BerriAI/litellm) for unified LLM access
- Inspired by chess ELO rating systems
- Special thanks to the open-source AI community

## ğŸ“ Support

- ğŸ› [Report Issues](https://github.com/yourusername/elo-benchmark/issues)
- ğŸ’¬ [Discussions](https://github.com/yourusername/elo-benchmark/discussions)
- ğŸ“§ Email: support@example.com

---

<div align="center">

**â­ Star this repo if you find it useful! â­**

Made with â¤ï¸ by the UNER team

</div>